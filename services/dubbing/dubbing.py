# -*- coding: utf-8 -*-
"""dubbing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x1Ql8n5kDeHBrF9swAgn7x8REBpy2QF4
"""

#Cell 1
!pip uninstall -y elevenlabs
!pip install --upgrade elevenlabs
!pip show elevenlabs

#Cell 2
import elevenlabs
print("Available functions and classes:", dir(elevenlabs))

#Cell 3

import requests
import json

# API Configuration
API_KEY = "sk_a7c92e88c2dd78a2fdea3386971adf3df6c60d541d0bad97"  # Exact key with lowercase prefix
HEADERS = {
    "Accept": "application/json",
    "xi-api-key": API_KEY
}

# First, let's list available voices
def list_voices():
    response = requests.get(
        "https://api.elevenlabs.io/v1/voices",
        headers=HEADERS
    )
    print("Response status:", response.status_code)
    if response.status_code == 200:
        voices = response.json()
        for voice in voices["voices"]:
            print(f"Voice: {voice['name']} (ID: {voice['voice_id']})")
    else:
        print("Error:", response.text)
        print("Headers used:", HEADERS)  # Debug info

# Test the voices endpoint
list_voices()

#Cell 4
def generate_speech(text, voice_id="EXAVITQu4vr4xnSDxMaL"):  # Sarah's voice ID
    url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"

    data = {
        "text": text,
        "model_id": "eleven_multilingual_v2",
        "voice_settings": {
            "stability": 0.5,
            "similarity_boost": 0.5
        }
    }

    response = requests.post(
        url,
        headers={"xi-api-key": API_KEY},
        json=data
    )

    if response.status_code == 200:
        with open("test_output.mp3", "wb") as f:
            f.write(response.content)
        print("Audio generated successfully!")
    else:
        print(f"Error {response.status_code}:", response.text)

# Test with a simple Spanish phrase
test_text = "Hola, esta es una prueba."
generate_speech(test_text)

#Cell 5 - Generate single natural dubbing
import requests

def create_natural_dub():
    print("Creating natural dubbing...")

    # Read the JSON file
    with open(filename, 'r', encoding='utf-8') as f:
        json_data = json.load(f)

    # Combine all text into one natural paragraph
    full_text = " ".join(segment["translated_text"].strip() for segment in json_data["segments"])
    print(f"Combined text: {full_text[:100]}...")

    # Generate single audio file with Daniel's voice
    url = f"https://api.elevenlabs.io/v1/text-to-speech/onwK4e9ZLuTAKqWW03F9"  # Daniel's voice

    data = {
        "text": full_text,
        "model_id": "eleven_multilingual_v2",
        "voice_settings": {
            "stability": 0.5,
            "similarity_boost": 0.5
        }
    }

    response = requests.post(
        url,
        headers={"xi-api-key": API_KEY},
        json=data
    )

    if response.status_code == 200:
        # Save as single audio file
        with open("natural_dub.mp3", "wb") as f:
            f.write(response.content)
        print("Natural dub created successfully!")

        # Download the file
        from google.colab import files
        files.download("natural_dub.mp3")
    else:
        print(f"Error generating audio:", response.text)

# Generate the natural dubbing
create_natural_dub()

#Cell 6 - Analyze accompaniment and create timing correlation
from pydub import AudioSegment
import librosa
import numpy as np

def analyze_accompaniment():
    print("Analyzing accompaniment...")

    # Load the accompaniment file
    y, sr = librosa.load('accompaniment.wav')

    # Detect onset strength
    onset_env = librosa.onset.onset_strength(y=y, sr=sr)

    # Find peaks (potential effects)
    peaks = librosa.util.peak_pick(onset_env,
                                 pre_max=3,  # Rise time
                                 post_max=3,  # Fall time
                                 pre_avg=3,   # Before onset
                                 post_avg=5,  # After onset
                                 delta=0.5,   # Threshold
                                 wait=10)     # Wait between peaks

    # Convert peak frames to timestamps
    peak_times = librosa.frames_to_time(peaks, sr=sr)

    # Analyze spectral contrast for each peak to determine if it's an effect or music
    effects = []
    for time in peak_times:
        # Get the frame index
        frame_idx = librosa.time_to_frames(time, sr=sr)

        # Calculate spectral contrast
        spec_contrast = librosa.feature.spectral_contrast(
            y=y[max(0, int(time*sr-sr/2)):int(time*sr+sr/2)],
            sr=sr
        )

        # High contrast usually indicates effects rather than music
        if np.mean(spec_contrast) > 20:  # Threshold to be adjusted
            effects.append({
                "time": float(time),
                "type": "effect",
                "contrast": float(np.mean(spec_contrast))
            })

    print(f"Found {len(effects)} potential sound effects")
    return effects

def correlate_effects_with_text():
    # Load original translation JSON
    with open(filename, 'r', encoding='utf-8') as f:
        translation_json = json.load(f)

    # Analyze natural_dub.mp3 for word timing
    y_dub, sr_dub = librosa.load('natural_dub.mp3')

    # Create new JSON structure correlating everything
    final_timing = {
        "metadata": {
            "source_file": "natural_dub.mp3",
            "accompaniment_file": "accompaniment.wav",
            "duration": librosa.get_duration(y=y_dub, sr=sr_dub)
        },
        "segments": []
    }

    # Get effects
    effects = analyze_accompaniment()

    # For each segment in the translation
    for segment in translation_json["segments"]:
        # Find corresponding effects
        segment_effects = [
            effect for effect in effects
            if segment["start"] <= effect["time"] <= segment["end"]
        ]

        # Add to new timing structure
        final_timing["segments"].append({
            "id": segment["id"],
            "text": segment["translated_text"],
            "original_timing": {
                "start": segment["start"],
                "end": segment["end"]
            },
            "effects": segment_effects
        })

    return final_timing

# Run the analysis
final_timing = correlate_effects_with_text()

# Save the result
with open('final_timing.json', 'w', encoding='utf-8') as f:
    json.dump(final_timing, f, indent=2, ensure_ascii=False)

print("\nAnalysis complete! Check final_timing.json for results.")

#Cell 8 - Analyze accompaniment types
import librosa
import numpy as np
from pydub import AudioSegment

def analyze_accompaniment_types():
    print("Analyzing accompaniment types...")

    # Load the audio
    y, sr = librosa.load('accompaniment.wav')

    # Parameters for analysis
    frame_length = 2048
    hop_length = 512

    # Features for classification
    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
    onset_env = librosa.onset.onset_strength(y=y, sr=sr)
    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)

    # Results storage
    sound_events = []

    # Detect significant events
    onset_peaks = librosa.util.peak_pick(onset_env,
                                       pre_max=3,
                                       post_max=3,
                                       pre_avg=3,
                                       post_avg=5,
                                       delta=0.5,
                                       wait=10)

    for peak in onset_peaks:
        time = librosa.frames_to_time(peak, sr=sr)
        frame_data = y[int(time*sr):int((time+0.5)*sr)]  # analyze 0.5s window

        if len(frame_data) == 0:
            continue

        # Feature extraction for classification
        centroid = np.mean(spectral_centroids[max(0, peak-5):min(len(spectral_centroids), peak+5)])
        rolloff = np.mean(spectral_rolloff[max(0, peak-5):min(len(spectral_rolloff), peak+5)])

        # Classification logic
        if librosa.feature.rms(y=frame_data)[0].mean() > 0.1:  # Significant energy
            if np.std(frame_data) > 0.2 and centroid > 3000:
                # Sharp, high-frequency content typically indicates effects
                event_type = "effect"
            elif tempo > 0 and abs(time - librosa.frames_to_time(beats, sr=sr)).min() < 0.1:
                # Regular beats indicate music
                event_type = "music"
            else:
                # Sustained, more uniform sound typically indicates ambiance
                event_type = "ambient"

            sound_events.append({
                "time": float(time),
                "type": event_type,
                "duration": 0.5,  # Default duration
                "features": {
                    "centroid": float(centroid),
                    "rolloff": float(rolloff)
                }
            })

    # Post-process to merge nearby music segments
    merged_events = []
    current_music = None

    for event in sorted(sound_events, key=lambda x: x["time"]):
        if event["type"] == "music":
            if current_music is None:
                current_music = event
            else:
                if event["time"] - (current_music["time"] + current_music["duration"]) < 0.5:
                    current_music["duration"] = event["time"] - current_music["time"] + event["duration"]
                else:
                    merged_events.append(current_music)
                    current_music = event
        else:
            if current_music is not None:
                merged_events.append(current_music)
                current_music = None
            merged_events.append(event)

    if current_music is not None:
        merged_events.append(current_music)

    # Print analysis
    print("\nSound Event Analysis:")
    print("--------------------")

    effects = [e for e in merged_events if e["type"] == "effect"]
    music_segments = [e for e in merged_events if e["type"] == "music"]
    ambient_segments = [e for e in merged_events if e["type"] == "ambient"]

    print(f"\nFound {len(effects)} distinct effects")
    print(f"Found {len(music_segments)} music segments")
    print(f"Found {len(ambient_segments)} ambient segments")

    # Save detailed analysis
    with open('sound_analysis.json', 'w') as f:
        json.dump({
            "effects": effects,
            "music_segments": music_segments,
            "ambient_segments": ambient_segments
        }, f, indent=2)

    return merged_events

# Run the analysis
events = analyze_accompaniment_types()

#Cell 9 - Handle ambient audio (flexible version)
from pydub import AudioSegment

def process_ambient_background(dub_path, ambient_path, output_path,
                             dub_volume_boost=2,
                             ambient_volume_reduction=15,
                             crossfade_duration=1000):
    """
    Process ambient background with any dubbed audio.

    Parameters:
    - dub_path: Path to the dubbed audio file (any language)
    - ambient_path: Path to the ambient background file
    - output_path: Where to save the final mix
    - dub_volume_boost: How much to boost dubbed speech (dB)
    - ambient_volume_reduction: How much to reduce ambient volume (dB)
    - crossfade_duration: Duration of crossfades in milliseconds
    """

    print("Processing ambient background...")

    # Load our files
    dubbed_audio = AudioSegment.from_file(dub_path)
    ambient_track = AudioSegment.from_file(ambient_path)

    # Get durations
    dub_length = len(dubbed_audio)
    ambient_length = len(ambient_track)

    print(f"Dub duration: {dub_length/1000:.2f}s")
    print(f"Ambient track duration: {ambient_length/1000:.2f}s")

    # Handle ambient track length
    if ambient_length > dub_length:
        # Trim the ambient track
        adjusted_ambient = ambient_track[:dub_length]
        print("Trimmed ambient track to match dub length")
    else:
        # Loop the ambient track if needed
        loops_needed = dub_length // ambient_length + 1
        adjusted_ambient = ambient_track * loops_needed
        adjusted_ambient = adjusted_ambient[:dub_length]
        print(f"Looped ambient track {loops_needed} times and trimmed")

    # Adjust volumes
    dubbed_audio = dubbed_audio + dub_volume_boost
    adjusted_ambient = adjusted_ambient - ambient_volume_reduction

    # Crossfade between loops if we had to loop
    if ambient_length < dub_length:
        adjusted_ambient = adjusted_ambient.fade_in(crossfade_duration).fade_out(crossfade_duration)

    # Mix
    final_mix = dubbed_audio.overlay(adjusted_ambient)

    # Export
    final_mix.export(output_path, format="mp3")
    print(f"\nFinal mix created at: {output_path}")
    return output_path

# Example usage
result_path = process_ambient_background(
    dub_path="natural_dub.mp3",
    ambient_path="accompaniment.wav",
    output_path="final_mix_ambient.mp3",
    dub_volume_boost=2,        # Adjust these parameters as needed
    ambient_volume_reduction=15,
    crossfade_duration=1000
)

# Download the result
from google.colab import files
files.download(result_path)

#Cell 9 - Mix dub and ambient with adjusted volumes
from pydub import AudioSegment

def mix_dub_and_ambient():
    print("Mixing dub with ambient (adjusted volumes)...")

    # Load our files
    dubbed_audio = AudioSegment.from_mp3("natural_dub.mp3")
    ambient_track = AudioSegment.from_wav("accompaniment.wav")

    # Get durations
    dub_length = len(dubbed_audio)
    ambient_length = len(ambient_track)

    print(f"Dub duration: {dub_length/1000:.2f}s")
    print(f"Ambient track duration: {ambient_length/1000:.2f}s")

    # Calculate how many times we need to loop ambient
    loops_needed = dub_length // ambient_length + 1
    adjusted_ambient = ambient_track * loops_needed
    adjusted_ambient = adjusted_ambient[:dub_length]

    # Adjust volumes - new values
    dubbed_audio = dubbed_audio + 2  # Boost speech slightly
    adjusted_ambient = adjusted_ambient - 5  # Reduced reduction from -15 to -5

    # Add crossfades between ambient loops
    crossfade_length = 1000  # 1 second crossfade
    adjusted_ambient = adjusted_ambient.fade_in(crossfade_length).fade_out(crossfade_length)

    # Mix
    final_mix = dubbed_audio.overlay(adjusted_ambient)

    # Export
    final_mix.export("final_mix_louder_ambient.mp3", format="mp3")
    print("\nFinal mix created!")

    # Download the result
    from google.colab import files
    files.download("final_mix_louder_ambient.mp3")

# Create the mix with louder ambient
mix_dub_and_ambient()

#Cell 10 - Create timing map
import json
from pydub import AudioSegment

def create_timing_map():
    print("Creating timing map...")

    # Load original timing from JSON
    with open(filename, 'r', encoding='utf-8') as f:
        original_data = json.load(f)

    # Load our final audio mix to get segment durations
    final_audio = AudioSegment.from_mp3("final_mix_louder_ambient.mp3")
    total_new_duration = len(final_audio) / 1000  # in seconds

    # Calculate timing ratio
    original_duration = max(segment["end"] for segment in original_data["segments"])
    stretch_ratio = total_new_duration / original_duration

    # Create timing map
    timing_map = {
        "metadata": {
            "original_duration": original_duration,
            "new_duration": total_new_duration,
            "stretch_ratio": stretch_ratio
        },
        "segments": []
    }

    # Map each segment
    for segment in original_data["segments"]:
        # Calculate new timings
        new_start = segment["start"] * stretch_ratio
        new_end = segment["end"] * stretch_ratio

        timing_map["segments"].append({
            "id": segment["id"],
            "text": segment["translated_text"],
            "original_timing": {
                "start": segment["start"],
                "end": segment["end"]
            },
            "new_timing": {
                "start": new_start,
                "end": new_end
            },
            "duration": {
                "original": segment["end"] - segment["start"],
                "new": new_end - new_start
            }
        })

    # Save timing map
    with open('video_timing_map.json', 'w', encoding='utf-8') as f:
        json.dump(timing_map, f, indent=2, ensure_ascii=False)

    # Print analysis
    print("\nTiming Analysis:")
    print(f"Original video duration: {original_duration:.2f}s")
    print(f"New audio duration: {total_new_duration:.2f}s")
    print(f"Stretch ratio: {stretch_ratio:.2f}x")
    print("\nSegment by segment breakdown:")

    for segment in timing_map["segments"]:
        print(f"\nSegment {segment['id']}:")
        print(f"Text: {segment['text'][:50]}...")
        print(f"Original: {segment['original_timing']['start']:.2f}s - {segment['original_timing']['end']:.2f}s")
        print(f"New: {segment['new_timing']['start']:.2f}s - {segment['new_timing']['end']:.2f}s")

# Generate timing map
create_timing_map()

#Cell 13 - Upload video file
from google.colab import files

print("Please upload your video file...")
uploaded = files.upload()

# Verify upload and rename if needed
if uploaded:
    original_name = list(uploaded.keys())[0]
    if original_name != "input_video.mp4":
        import os
        os.rename(original_name, "input_video.mp4")
        print(f"Renamed {original_name} to input_video.mp4")
    print("Video upload successful!")
else:
    print("No file was uploaded")

#Cell 16 - Create final dubbed video using both timing maps
from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips
import json

def create_final_dubbed_video():
    print("Creating final dubbed video...")

    try:
        # Load both timing maps
        print("\nLoading timing maps...")
        with open('video_timing_map.json', 'r') as f:
            video_timing = json.load(f)
        with open('final_timing.json', 'r') as f:
            audio_timing = json.load(f)

        # Load files
        print("\nLoading media files...")
        video = VideoFileClip("input_video.mp4")
        mixed_audio = AudioFileClip("final_mix_louder_ambient.mp3")

        print(f"Input video duration: {video.duration:.2f}s")
        print(f"Mixed audio duration: {mixed_audio.duration:.2f}s")

        # Process video segments according to audio timing
        print("\nProcessing video segments...")
        clips = []

        for segment in video_timing["segments"]:
            print(f"\nSegment {segment['id']}:")
            print(f"Text: {segment['text'][:50]}...")

            # Original video timing
            v_start = segment["original_timing"]["start"]
            v_end = segment["original_timing"]["end"]

            # Find corresponding audio timing
            a_start = segment["new_timing"]["start"]
            a_end = segment["new_timing"]["end"]

            # Calculate stretch factor for this segment
            original_duration = v_end - v_start
            new_duration = a_end - a_start
            stretch_factor = new_duration / original_duration

            print(f"Video timing: {v_start:.2f}s -> {v_end:.2f}s ({original_duration:.2f}s)")
            print(f"Audio timing: {a_start:.2f}s -> {a_end:.2f}s ({new_duration:.2f}s)")
            print(f"Stretch factor: {stretch_factor:.2f}x")

            # Cut and adjust video segment
            clip = video.subclip(v_start, v_end)
            adjusted = clip.set_duration(new_duration)
            clips.append(adjusted)

        print("\nConcatenating video segments...")
        final_video = concatenate_videoclips(clips)

        print("\nAdding mixed audio (unchanged)...")
        final_video = final_video.set_audio(mixed_audio)

        output_filename = "final_dubbed_video.mp4"
        print(f"\nWriting final video to {output_filename}...")
        final_video.write_videofile(
            output_filename,
            codec='libx264',
            audio_codec='aac',
            temp_audiofile='temp-audio.m4a',
            remove_temp=True,
            fps=24,
            verbose=False
        )

        # Clean up
        video.close()
        mixed_audio.close()
        final_video.close()

        print(f"\nSuccess! Final video saved as: {output_filename}")

        # Download the result
        from google.colab import files
        files.download(output_filename)

    except Exception as e:
        print(f"\nError during processing: {str(e)}")

# Create the final dubbed video
create_final_dubbed_video()

#Cell 13 - Video upload with status reporting
from google.colab import files
import IPython.display as display
import time

def upload_video():
    display.clear_output()
    print("Starting upload process...")
    print("Waiting for file selection dialog...")
    print("(You should see a 'Choose Files' button above)")

    try:
        print("\nStatus: Waiting for file selection...")
        uploaded = files.upload()

        if uploaded:
            print("\nFile received!")
            original_name = list(uploaded.keys())[0]
            print(f"Uploaded file name: {original_name}")

            if original_name != "input_video.mp4":
                import os
                print(f"Renaming file to input_video.mp4...")
                os.rename(original_name, "input_video.mp4")
                print("Rename successful!")

            print("\nChecking file...")
            import os
            if os.path.exists("input_video.mp4"):
                size_mb = os.path.getsize("input_video.mp4") / (1024*1024)
                print(f"File size: {size_mb:.2f} MB")
                print("\nVideo upload and processing COMPLETE!")
                return True
            else:
                print("Error: File not found after upload")
                return False
        else:
            print("\nNo file was uploaded")
            return False

    except Exception as e:
        print(f"\nError during upload: {str(e)}")
        return False

# Try to upload with status updates
print("Initializing upload process...")
time.sleep(1)  # Give time for initial message
upload_successful = upload_video()

# Final status check
if upload_successful:
    print("\nFinal Status: Ready to proceed with video processing")
else:
    print("\nFinal Status: Upload failed or was cancelled")

#Cell 12 - Check files and paths
import os

def check_files():
    print("Checking required files...")

    files_needed = {
        "Video": "input_video.mp4",
        "Audio mix": "final_mix_louder_ambient.mp3",
        "Timing map": "video_timing_map.json"
    }

    for file_type, filename in files_needed.items():
        if os.path.exists(filename):
            file_size = os.path.getsize(filename) / (1024*1024)  # Convert to MB
            print(f"✓ {file_type}: {filename} ({file_size:.2f} MB)")
        else:
            print(f"✗ {file_type}: {filename} not found")

    # List all files in current directory
    print("\nAll files in current directory:")
    for file in os.listdir():
        print(f"- {file}")

check_files()

#Cell 5 - Upload files sequentially
from google.colab import files
import os
import json

def upload_single_file(file_name, description):
    print(f"\nPlease upload: {file_name}")
    print(f"Description: {description}")

    uploaded = files.upload()

    if uploaded:
        uploaded_name = list(uploaded.keys())[0]
        if uploaded_name != file_name:
            print(f"Renaming {uploaded_name} to {file_name}")
            os.rename(uploaded_name, file_name)

        size_mb = os.path.getsize(file_name) / (1024*1024)
        print(f"✓ Uploaded {file_name} ({size_mb:.2f} MB)")
        return True
    return False

def verify_json_file(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if 'segments' in data:
                print(f"  ✓ Valid JSON with {len(data['segments'])} segments")
            else:
                print(f"  ⚠ JSON structure different than expected")
        return True
    except Exception as e:
        print(f"  ✗ Error reading JSON: {str(e)}")
        return False

def upload_all_files():
    required_files = [
        ("final_mix_louder_ambient.mp3", "Spanish dub with ambient background"),
        ("accompaniment.wav", "Original ambient track"),
        ("vocals_transcript_ES_translation.json", "Spanish translation with timings"),
        ("final_timing.json", "Final timing map")
    ]

    uploaded_files = []

    for file_name, description in required_files:
        if os.path.exists(file_name):
            size_mb = os.path.getsize(file_name) / (1024*1024)
            print(f"\n✓ Already have {file_name} ({size_mb:.2f} MB)")
            uploaded_files.append(file_name)
            if file_name.endswith('.json'):
                verify_json_file(file_name)
        else:
            print("\n" + "="*50)
            if upload_single_file(file_name, description):
                uploaded_files.append(file_name)
                if file_name.endswith('.json'):
                    verify_json_file(file_name)

            proceed = input("\nPress Enter to continue with next file (or 'q' to quit): ")
            if proceed.lower() == 'q':
                break

    print("\nFinal status:")
    for file_name, _ in required_files:
        if file_name in uploaded_files:
            size_mb = os.path.getsize(file_name) / (1024*1024)
            print(f"✓ {file_name} ({size_mb:.2f} MB)")
        else:
            print(f"✗ Missing {file_name}")

    return len(uploaded_files) == len(required_files)

# Start the upload process
files_ready = upload_all_files()

if files_ready:
    print("\n✓ All files uploaded and verified successfully!")
else:
    print("\n⚠ Some files are missing. Please check the status above.")

#Cell 6b - Recreate final timing JSON
import json
from pydub import AudioSegment

def create_final_timing():
    print("Creating final timing map...")

    # Load translation file
    with open('vocals_transcript_ES_translation.json', 'r', encoding='utf-8') as f:
        translation_data = json.load(f)

    # Load audio to get final duration
    audio = AudioSegment.from_mp3("final_mix_louder_ambient.mp3")
    total_duration = len(audio) / 1000  # Convert to seconds

    # Calculate timing ratio
    original_duration = max(segment["end"] for segment in translation_data["segments"])
    stretch_ratio = total_duration / original_duration

    # Create timing map
    timing_map = {
        "metadata": {
            "original_duration": original_duration,
            "new_duration": total_duration,
            "stretch_ratio": stretch_ratio
        },
        "segments": []
    }

    # Map each segment
    for segment in translation_data["segments"]:
        new_start = segment["start"] * stretch_ratio
        new_end = segment["end"] * stretch_ratio

        timing_map["segments"].append({
            "id": segment["id"],
            "text": segment["translated_text"],
            "original_timing": {
                "start": segment["start"],
                "end": segment["end"]
            },
            "new_timing": {
                "start": new_start,
                "end": new_end
            }
        })

    # Save timing map
    with open('final_timing.json', 'w', encoding='utf-8') as f:
        json.dump(timing_map, f, indent=2, ensure_ascii=False)

    print("\nTiming map created!")
    print(f"Original duration: {original_duration:.2f}s")
    print(f"New duration: {total_duration:.2f}s")
    print(f"Stretch ratio: {stretch_ratio:.2f}x")

    # Print first segment as sample
    print("\nSample segment timing:")
    segment = timing_map["segments"][0]
    print(f"Segment 0: {segment['text'][:50]}...")
    print(f"Original: {segment['original_timing']['start']:.2f}s -> {segment['original_timing']['end']:.2f}s")
    print(f"New: {segment['new_timing']['start']:.2f}s -> {segment['new_timing']['end']:.2f}s")

# Create the timing map
create_final_timing()

#Cell 6a - Install pydub
!pip install pydub

#Cell 9 - Create final dubbed video with proper slow motion
from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips
from moviepy.video.fx import all as vfx
import json

def create_final_dubbed_video():
    print("Creating final dubbed video with proper slow motion...")

    try:
        # Load timing map
        print("\nLoading timing map...")
        with open('final_timing.json', 'r') as f:
            timing_map = json.load(f)

        # Load video and audio
        print("Loading video and audio...")
        video = VideoFileClip("input_video.mp4")
        mixed_audio = AudioFileClip("final_mix_louder_ambient.mp3")

        print(f"Video duration: {video.duration:.2f}s")
        print(f"Audio duration: {mixed_audio.duration:.2f}s")

        # Process video segments
        print("\nProcessing video segments...")
        clips = []

        for segment in timing_map["segments"]:
            print(f"\nSegment {segment['id']}:")

            # Get timings
            v_start = segment["original_timing"]["start"]
            v_end = segment["original_timing"]["end"]
            original_duration = v_end - v_start
            new_duration = segment["new_timing"]["end"] - segment["new_timing"]["start"]

            # Calculate slow motion factor
            slow_factor = new_duration / original_duration

            print(f"Segment {segment['id']}: {v_start:.2f}s -> {v_end:.2f}s")
            print(f"Original duration: {original_duration:.2f}s")
            print(f"Target duration: {new_duration:.2f}s")
            print(f"Slow factor: {slow_factor:.2f}x")

            # Cut and properly slow down video segment
            clip = video.subclip(v_start, v_end)
            # Use proper slow motion with frame blending
            slowed = clip.fx(vfx.speedx, factor=1/slow_factor)

            # Verify durations
            print(f"Result duration: {slowed.duration:.2f}s")

            clips.append(slowed)

        print("\nConcatenating video segments...")
        final_video = concatenate_videoclips(clips)

        print("\nAdding mixed audio...")
        final_video = final_video.set_audio(mixed_audio)

        output_filename = "final_dubbed_video_slowmo.mp4"
        print(f"\nWriting final video to {output_filename}...")
        final_video.write_videofile(
            output_filename,
            codec='libx264',
            audio_codec='aac',
            temp_audiofile='temp-audio.m4a',
            remove_temp=True,
            fps=24,
            verbose=False
        )

        # Clean up
        video.close()
        mixed_audio.close()
        final_video.close()

        print(f"\nSuccess! Final video saved as: {output_filename}")

        # Download the result
        from google.colab import files
        files.download(output_filename)

    except Exception as e:
        print(f"\nError during processing: {str(e)}")

# Create the final dubbed video
create_final_dubbed_video()

#Cell 8 - Download the video file
from google.colab import files

print("Initiating download of final video...")
try:
    files.download('final_dubbed_video.mp4')
    print("Download initiated. Check your browser's download folder.")
except Exception as e:
    print(f"Error during download: {str(e)}")

#Cell 10 - Prepare files for existing repository
import os
import shutil
from google.colab import files

def organize_files_for_repo():
    print("Organizing files for youtube_dubbing_tool repository...")

    # Define target paths matching your repository structure
    target_paths = {
        'final_mix_louder_ambient.mp3': '/services/dubbing/output/',
        'final_dubbed_video_slowmo.mp4': '/services/dubbing/output/',
        'final_timing.json': '/services/dubbing/output/',
        'vocals_transcript_ES_translation.json': '/services/translation/output/',
        'input_video.mp4': '/services/dubbing/input/'  # Adding input video
    }

    # Create temp directory structure
    base_dir = 'repo_output'
    for path in target_paths.values():
        os.makedirs(f"{base_dir}{path}", exist_ok=True)

    # Copy files with status reporting
    print("\nCopying files:")
    for file, path in target_paths.items():
        if os.path.exists(file):
            dest_path = f"{base_dir}{path}{file}"
            shutil.copy2(file, dest_path)
            size_mb = os.path.getsize(file) / (1024*1024)
            print(f"✓ Copied {file} ({size_mb:.2f} MB) to {path}")
        else:
            print(f"✗ Warning: {file} not found")

    # Create zip
    print("\nCreating zip file...")
    shutil.make_archive('repo_output', 'zip', base_dir)

    # Download
    print("Initiating download...")
    files.download('repo_output.zip')

    print("\nFiles organized for your repository!")
    print("\nNext steps:")
    print("1. Unzip repo_output.zip")
    print("2. Copy files to your local repository clone:")
    print("   cd /path/to/youtube_dubbing_tool")
    print("3. Commit and push changes:")
    print("   git add services/dubbing/output/* services/translation/output/*")
    print('   git commit -m "Add generated files from Colab"')
    print("   git push origin main")

# Organize files
organize_files_for_repo()